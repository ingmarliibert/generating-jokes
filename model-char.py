# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k3cupMS-rv2_YDL-EQnDZPaFtqf0K3vY

# Mounting drive

Create shortcut to project folder in your main drive.
"""

data_path = "reddit_jokes.json"

"""# Text generation with an RNN

Tutorial: https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/

## Setup

### Import TensorFlow and other libraries
"""

import tensorflow as tf
from tensorflow.keras.layers.experimental import preprocessing
from tensorflow import keras

import numpy as np
import os
import time
import json
import pandas as pd
import random

from pickle import dump
from keras.preprocessing.text import Tokenizer
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import GRU
from keras.layers import Embedding
from keras.callbacks import ModelCheckpoint

from functools import reduce

"""## Process the text

#### Reddit dataset: cleaning

Cleaning the dataset:
- Remove anything from the post following \"edit: \"
- Remove duplicate posts
- Create a \"joke\" column to the df by either combining titles with bodies or just returning bodies(if it contains the title)
"""

number_of_jokes = 10000

# Cleaning for jokes dataset
def clean_df(df):
    # Remove content after edit
    df["title"] = df["title"].str.replace(r'edit:.*', '')
    df["body"] = df["body"].str.replace(r'edit:.*', '')
    
    # Creating "joke" column
    df["joke"] = np.where(df["title"].str[:10] != df["body"].str[:10], df["title"] + " " + df["body"], df["body"])
    df["joke"] = df["joke"] + "‚ùå"
    
    return df

# Read json, that has reddit submissions as "title" and "body", combine them into column "joke" and 
# remove duplicates.
def read(json_filename):
    df = pd.read_json(path_or_buf=json_filename,orient='records',compression="infer")
    print("All jokes len", len(df))
    df = clean_df(df.iloc[:number_of_jokes])
    print("Loaded", number_of_jokes)
    
    return df

jokes_df = read(data_path)

"""### Vectorize the text"""

jokes_list = jokes_df['joke'].to_numpy()
tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)
tokenizer.fit_on_texts(jokes_list)
dump(tokenizer, open('tokenizer-char.pkl', 'wb'))


"""### The prediction task"""

jokes_without_word = []
word_without_joke = []
sequence_length = 100

for j, joke in enumerate(jokes_list[:number_of_jokes]):
  joke_words = tokenizer.texts_to_sequences([joke])
  joke_words = list(reduce(lambda a, b: a + b, joke_words))
  if j % 1000 == 0:
    print(j)
  if len(joke_words) < 2:
    continue
  if len(joke_words) <= sequence_length:
    word = joke_words[-1]
    seq = joke_words[:len(joke_words) - 1]
    seq = [0] * (sequence_length - len(seq)) + seq
    jokes_without_word.append(np.array(seq))
    word_without_joke.append(word)
  else:
    for i in range(len(joke_words)):
      if len(joke_words) - i < sequence_length + 1:
        break
      window = joke_words[i:i + sequence_length + 1]
      word = window[-1]
      seq = window[:len(window) - 1]
      jokes_without_word.append(np.array(seq))
      word_without_joke.append(word)

print(len(jokes_without_word), "sequences")

for joke in jokes_without_word:
  if len(joke) != sequence_length:
    print(len(joke), joke)

X = np.array(jokes_without_word)
y = word_without_joke
vocab_size = len(tokenizer.word_index) + 1
y = to_categorical(y, num_classes=vocab_size)

# define model
model = Sequential()
model.add(Embedding(vocab_size, 50, input_length=sequence_length))
model.add(GRU(64))
model.add(Dense(64, activation='relu'))
model.add(Dense(vocab_size, activation='softmax'))
print(model.summary())
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
checkpoint = ModelCheckpoint("model-char-best-accuracy.hdf5", monitor='accuracy', verbose=1, save_best_only=True, mode='max')
model.fit(X, y, batch_size=64, epochs=200, validation_split=0.05, callbacks=[checkpoint], use_multiprocessing=True, workers=16)
model.save('model-char.h5')